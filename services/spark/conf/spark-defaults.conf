# Spark Default Configuration
# spark.master                     spark://spark-master:7077
spark.dynamicAllocation.enabled=true
spark.shuffle.service.enabled=false
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=2
spark.dynamicAllocation.shuffleTracking.enabled=true

spark.driver.extraClassPath      /opt/bitnami/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar:/opt/bitnami/spark/jars/mysql-connector-java-8.0.21.jar
spark.executor.extraClassPath    /opt/bitnami/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar:/opt/bitnami/spark/jars/mysql-connector-java-8.0.21.jar

# Hive Metastore Integration
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir=/user/hive/warehouse
spark.hive.metastore.uris=thrift://hive-metastore:9083

# Thrift Server Configuration
spark.sql.hive.thriftServer.singleSession=true
spark.sql.hive.thriftServer.hiveServer2.authentication=NONE

# Performance Configuration
spark.executor.memory=1536m
spark.driver.memory=512m
spark.executor.cores=2
spark.driver.cores=1
spark.sql.shuffle.partitions=4
spark.default.parallelism=4

# Memory Management
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5
spark.executor.memoryOverhead=256m
spark.driver.memoryOverhead=256m

# JDBC Read Optimization
spark.sql.jdbc.batchsize=10000
spark.sql.jdbc.fetchsize=10000
spark.sql.jdbc.batchFetchSize=10000
spark.sql.jdbc.batchInsertSize=10000
spark.sql.jdbc.batchTimeout=10000

# Garbage Collection
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=2 -XX:MaxGCPauseMillis=200

# iceberg Configuration
spark.sql.extensions                        org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg                   org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type              hive
spark.sql.catalog.iceberg.uri               thrift://hive-metastore:9083

spark.sql.catalog.spark_catalog             org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type        hive

# for monitoring with prometheus
spark.ui.prometheus.enabled                 true

# # this for spark history server
# spark.eventLog.enabled=true
# spark.eventLog.dir=hdfs://namenode:8020/spark-logs
# spark.history.fs.logDirectory=hdfs://namenode:8020/spark-events
# spark.history.fs.cleaner.enabled=true
# spark.history.fs.cleaner.interval=1d
# spark.history.fs.cleaner.maxAge=7d
spark.sql.autoBroadcastJoinThreshold 10485760
# reduce number of small files
spark.sql.files.maxPartitionBytes 128MB
# enable to reduce network I/O
spark.sql.adaptive.localShuffleReader.enabled true
spark.executor.processTreeMetrics.enabled false
spark.executor.metrics.pollingInterval 0
spark.sql.authorization.enabled true
spark.hadoop.hive.security.authorization.manager org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizerFactory
spark.hadoop.hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator
spark.hadoop.ranger.plugin.hive.service.name hive
spark.hadoop.ranger.plugin.hive.policy.rest.url http://ranger-admin:6080
spark.hadoop.ranger.plugin.hive.policy.rest.username admin
spark.hadoop.ranger.plugin.hive.policy.rest.password rangeradmin1
spark.hadoop.hive.server2.enable.doAs false
spark.hadoop.xasecure.audit.destination.elasticsearch true
spark.hadoop.xasecure.audit.elasticsearch.uris ranger-es:9200
spark.hadoop.xasecure.audit.elasticsearch.user elasticsearch
spark.hadoop.xasecure.audit.elasticsearch.password elasticsearch